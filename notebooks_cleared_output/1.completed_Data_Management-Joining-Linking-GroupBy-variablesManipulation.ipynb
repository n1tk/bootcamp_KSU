{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Per Dr. Priestley's class STAT8330: Data - there are two sets of files here: \n",
    "* CLEAN and NOT CLEAN.\n",
    "\n",
    "* The NOT CLEAN files are raw and have all of the coded values and the CLEAN files have the coded values imputed with the median.\n",
    "#### All files can be merged on Matchkey."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np #library for math operations\n",
    "import pandas as pd #this is how I usually import pandas\n",
    "import sys #only needed to determine Python version number\n",
    "import matplotlib #only needed to determine Matplotlib version number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Python version ' + sys.version)\n",
    "print('Pandas version ' + pd.__version__)\n",
    "print('Matplotlib version ' + matplotlib.__version__)\n",
    "print('Numpy version ' + np.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Data \n",
    "Location file is used the URL, from pandas version 0.19.2 the URL can be directly passed and if is used the local source the read can be perfomed as: \n",
    "Location = r'C:\\Users\\sb0709\\bootcamp_KSU\\blob\\master\\Data\\CLEAN1A.csv' # r is required so can read the slashes and is a standard in python with the strings read. \n",
    "\n",
    "### Tutorial source http://nbviewer.jupyter.org/urls/bitbucket.org/hrojas/learn-pandas/raw/master/lessons/01%20-%20Lesson.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull the data from remote location: here is my github \"Data\" folder\n",
    "# Note: if you read the http files than read the file content and not the html because you'll get issue and not knowing where comes from.\n",
    "# docs source: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html\n",
    "\n",
    "url_CLEAN1A = \"https://raw.githubusercontent.com/sb0709/bootcamp_KSU/master/Data/CLEAN1A.csv\"\n",
    "url_CLEAN1B = \"https://raw.githubusercontent.com/sb0709/bootcamp_KSU/master/Data/CLEAN1B.csv\"\n",
    "url_CLEAN1C = \"https://raw.githubusercontent.com/sb0709/bootcamp_KSU/master/Data/CLEAN1C.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe from the .csv files or simply read in csv files. \n",
    "\n",
    "df_CLEAN1A = pd.read_csv(url_CLEAN1A,sep=',')\n",
    "df_CLEAN1B = pd.read_csv(url_CLEAN1B,sep=',')\n",
    "df_CLEAN1C = pd.read_csv(url_CLEAN1C,sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_CLEAN1A.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_CLEAN1B.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_CLEAN1C.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_CLEAN1A.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repeated part from morning track: Check the data for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum() is ussed to get the total of each value missing per collumn.\n",
    "\n",
    "print(\"Print content for df_CLEAN1A:\",'\\n', df_CLEAN1A.isnull().sum()) \n",
    "print('\\n')\n",
    "print(\"Print content for df_CLEAN1B:\",'\\n', df_CLEAN1B.isnull().sum())\n",
    "print('\\n')\n",
    "print(\"Print content for df_CLEAN1C:\",'\\n', df_CLEAN1C.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repeated part from morning track: Cleaning / filling missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_NOTCLEAN1A = \"https://raw.githubusercontent.com/sb0709/bootcamp_KSU/master/Data/NOTCLEAN1A.csv\"\n",
    "df_NOTCLEAN1A = pd.read_csv(url_NOTCLEAN1A,sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the structure of the dataframe\n",
    "\n",
    "df_NOTCLEAN1A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling missing values with function: fillna() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_NOTCLEAN1A.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_NOTCLEAN1A['AGE'].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeated part from morning track: fill with media or mean example so can match the orginal cleaned data. \n",
    "\n",
    "df_imputed_median_NOTCLEAN1A = df_NOTCLEAN1A.fillna(df_NOTCLEAN1A.median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imputed_median_NOTCLEAN1A.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeated part from morning track: impute with the mean \n",
    "\n",
    "df_imputed_mean_NOTCLEAN1A = df_NOTCLEAN1A.fillna(df_NOTCLEAN1A.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imputed_mean_NOTCLEAN1A.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with dates sample:\n",
    "\n",
    "source documentation: https://pandas.pydata.org/pandas-docs/stable/timeseries.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df_imputed_mean_NOTCLEAN1A.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the column timestamp with Hours for our dataframe\n",
    "\n",
    "df2['timestamp'] = pd.date_range('8/8/2018', periods=len(df2['MATCHKEY']), freq='D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting to Timestamps\n",
    "\n",
    "df2['timestamp'] = pd.to_datetime(df2['timestamp'], format='%d/%b/%Y:%H:%M:%S +0000', utc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_CLEAN1A.info()\n",
    "\n",
    "# provides info related to the dataframe df_CLEAN1A as follow: 1k observations, each column has datatype of numeric(int64) and also how much memory is using in our machine. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check the max() AGE value in our dataset and also the min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The eldery person in our dataset is 92\n",
    "\n",
    "df_CLEAN1A['AGE'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The youngest person in our dataset is 92\n",
    "\n",
    "df_CLEAN1A['AGE'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the categories using cut with argument bins\n",
    "labels = ['AG_0_30','AG_30_50','AG_50_70','AG_70_UP']\n",
    "df_CLEAN1A['AGE_groups'] = pd.cut(df_CLEAN1A['AGE'], bins=[0, 30, 50, 70, 100], labels=labels)\n",
    "\n",
    "#df_CLEAN1A['AGE_groups'] = pd.cut(df_CLEAN1A['AGE'], bins=[0, 30, 50, 70, 100], labels=False)\n",
    "#labels = np.array('[1,2,3,4,5,6]'.split())\n",
    "#df_CLEAN1A['AGE_groups'] = labels[df_CLEAN1A['AGE_groups']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_CLEAN1A.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to categorical the AGE_groups collumn.\n",
    "\n",
    "df_CLEAN1A['AGE_groups'] = df_CLEAN1A['AGE_groups'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_CLEAN1A.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set(style=\"darkgrid\")\n",
    "ax = sns.countplot(x=\"AGE_groups\", data=df_CLEAN1A)\n",
    "\n",
    "#doc source https://seaborn.pydata.org/generated/seaborn.countplot.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge, join, and concatenate\n",
    "### Join dataframes in 1 single dataframe by MATCHKEY\n",
    "###### doc source: https://pandas.pydata.org/pandas-docs/stable/merging.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### My prefered option if Database-style DataFrame joining/merging because as Data Scientist/Data Engineer we work with multiple sources and one comun and widelly used is the databases(any SQL, noSQL and even SAS has Proc SQL for working is data so is cross platform the \"Concept\").\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Users who are familiar with SQL but new to pandas might be interested in a comparison with SQL.\n",
    "\n",
    "pandas provides a single function, merge(), as the entry point for all standard database join operations between DataFrame objects:\n",
    "\n",
    "```python\n",
    "pd.merge(left, right, how='inner', on=None, left_on=None, right_on=None,\n",
    "         left_index=False, right_index=False, sort=True,\n",
    "         suffixes=('_x', '_y'), copy=True, indicator=False,\n",
    "         validate=None)\n",
    "```\n",
    "\n",
    "\n",
    "![alt text](https://github.com/sb0709/bootcamp_KSU/blob/master/pictures/merge_methods.png?raw=true?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example for merge/join 2 dataframes as per documentation.\n",
    "```python\n",
    "data = pd.merge(df_CLEAN1A, df_CLEAN1B on='MATCHKEY', how='inner', indicator='indicator_column')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in our case we have more than 2 dataframes so the following code will merge more than \n",
    "from functools import reduce\n",
    "\n",
    "dfs = [df_CLEAN1A, df_CLEAN1B, df_CLEAN1C] # lift of the dataframes\n",
    "data = reduce(lambda left,right: pd.merge(left,right,on='MATCHKEY', how='inner'), dfs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# content of the merge of 3 dataframes\n",
    "\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use catplot() to combine a countplot() and a FaceGrid()\n",
    "\n",
    "g = sns.catplot(x=\"AGE_groups\", col=\"goodbad\",\n",
    "                 data=data, kind=\"count\",\n",
    "                 height=5, aspect=.9);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering, Sorting and groupBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select observations where the AGE is greater than 70\n",
    "age_up70 = data[data['AGE'] > 70]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_up70.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the shape of the age_up70 dataframe\n",
    "\n",
    "age_up70.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slicing the columns of the dataframe data\n",
    "sl_data = data[['goodbad','TRADES',  'AGE_groups']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The shape of the dataframe is:\", sl_data.shape)\n",
    "print()\n",
    "print(\"Print the head of the new created dataframe:\", '\\n', sl_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping the data (using columns and applying functions like mean, sum, count)\n",
    "\n",
    "##### pandas.DataFrame.groupby \n",
    "\n",
    "```python\n",
    "DataFrame.groupby(by=None, axis=0, level=None, as_index=True, sort=True, group_keys=True, squeeze=False, observed=False, **kwargs)\n",
    "```\n",
    "\n",
    "##### doc source https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# groupBy function usage with the columns we group to become as the index.\n",
    "\n",
    "g_goodbad_index = sl_data.groupby(['goodbad','AGE_groups']).sum()\n",
    "g_goodbad_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# groupBy function usage if you want to not have the columns you are groupBy to become the index.\n",
    "\n",
    "g_goodbad_not_index = sl_data.groupby(['goodbad','AGE_groups'], as_index=False).sum()\n",
    "g_goodbad_not_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd #location of the current working directory. \n",
    "\n",
    "# path for writing to csv the merged dataset: /home/sb0709/github_repos/bootcamp_ksu/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write the dataframe to csv file function pandas: pandas.DataFrame.to_csv\n",
    "\n",
    "```python\n",
    "DataFrame.to_csv(path_or_buf=None, sep=', ', na_rep='', float_format=None, columns=None, header=True, index=True, index_label=None, mode='w', encoding=None, compression=None, quoting=None, quotechar='\"', line_terminator='\\n', chunksize=None, tupleize_cols=None, date_format=None, doublequote=True, escapechar=None, decimal='.')\n",
    "\n",
    "Write DataFrame to a comma-separated values (csv) file: we need so we can later import for anaylis the whole dataset. \n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we need so we can later import for anaylis the whole dataset. \n",
    "\n",
    "data.to_csv('/home/sb0709/github_repos/bootcamp_ksu/Data/data.csv', sep = ',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary and Q&A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python36 (bootcamp_conda_ksu)",
   "language": "python",
   "name": "bootcamp_conda_ksu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
